% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kl.R
\name{KL_mods}
\alias{KL_mods}
\title{Compute KL divergence of one model from a second}
\usage{
KL_mods(mod1, mod2)
}
\arguments{
\item{mod1}{true model (list with fields mu and Sigma)}

\item{mod2}{other model}
}
\value{
KL divergence of mod2 (candidate) from mod1 (true model), in bits.
}
\description{
The KL divergence of mod2 from mod1 is the cost (in bits) of encoding data
drawn from the distribution of the true model (\code{mod1}) using a code that
is optimized for another model's distribution (\code{mod2}). That is, it
measures how much it hurts to think that data is coming from \code{mod2} when
it's actually generated from \code{mod1}.
}
